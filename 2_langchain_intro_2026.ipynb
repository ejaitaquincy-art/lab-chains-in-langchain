{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU7fO1sAM33A"
      },
      "source": [
        "# Building AI Workflows with LangChain: Prompts, Chains, and LLM Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHHATMjTM33B"
      },
      "source": [
        "we’ll start with the basics behind prompt templates and LLMs. We’ll also explore two LLM options available from the library, using models from Hugging Face Hub or OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN8dU50WM33B"
      },
      "source": [
        "###  Note on LangChain 1.0 Syntax\n",
        "\n",
        "This notebook uses **LangChain 1.0** syntax. Key changes from earlier versions include:\n",
        "- Imports are organized into separate packages: `langchain_openai`, `langchain_community`, and `langchain_core`\n",
        "- `LLMChain` is deprecated in favor of **LCEL (LangChain Expression Language)** using the `|` pipe operator\n",
        "- For complex agent workflows, consider using **LangGraph** which provides better orchestration capabilities\n",
        "\n",
        "**Note:** Some legacy chain examples are shown for educational purposes, but the recommended approach is LCEL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To0cTjhBFT0V"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_O9D7TUM33C"
      },
      "source": [
        "# LangChain 1.0 uses separate packages for different providers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain_openai langchain_community langchain_core\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PeUFqKUNB2Y",
        "outputId": "745135ce-5759-48bd-8b16-654762daf93c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (1.1.10)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (1.2.16)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.23.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.47)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (9.1.4)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.13.1)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.7.6)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (26.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.14.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (4.67.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cee7L_56FdMF",
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:05:13.335771Z",
          "start_time": "2026-02-18T11:05:11.421547Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd140578-2c1e-40bf-cb62-9af83785536d"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# LangChain 1.0 imports - organized by package\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "\n",
        "# Core components\n",
        "from langchain_core.runnables import Runnable, RunnableSequence, RunnableParallel\n",
        "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
        "\n",
        "# Legacy chains (moved to langchain_classic in LangChain 1.2+)\n",
        "from langchain_classic.chains import SimpleSequentialChain, LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
        "\n",
        "import inspect\n",
        "import re\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "print(OPENAI_API_KEY)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"SERPAPI_API_KEY\"] = userdata.get(\"SERPAPI_API_KEY\")"
      ],
      "metadata": {
        "id": "ejhYwBKrOV4c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:05:18.000669Z",
          "start_time": "2026-02-18T11:05:17.842204Z"
        },
        "id": "JDA1cS7oM33C"
      },
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.7\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghdws72ROs9I"
      },
      "source": [
        "### Prompts and Prompt templates ✏️\n",
        "\n",
        "Before understanding the concept of prompt templates it is important that you must be aware about what does prompt actually means. Prompt is simply a textual instruction which we give to a model to provide some specific output.\n",
        "\n",
        "To better understand this let us assume that we want some outline about tennis. So for this our prompt could something be like \"Write me an outline on Tennis\". But what if we want to again want to get some outline about some other sport let say cricket. In such kind of scenarios the naive approach would be to simply rewrite the prompt with updated sport.\n",
        "\n",
        "But, if you are an AI Engineer you would be aware about the concept of code reproducibility and in the above mentioned naive approach this concept is getting violated as for every different city we are forced to rewrite the entire prompts with updated sport, so to make the process of creating the prompts efficient we use prompts templates.\n",
        "\n",
        "*Prompt templates are like ready-made templates which contains contextual information about the input parameter, where input parameter is simply the input provided by the end user. The below mentioned code snipped will help you understand how we can create prompts efficiently.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM0K6i1rM33D"
      },
      "source": [
        "## Structure of a Prompt\n",
        "\n",
        "A prompt can consist of multiple components:\n",
        "\n",
        "* Instructions\n",
        "* External information or context\n",
        "* User input or query\n",
        "* Output indicator\n",
        "\n",
        "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
        "\n",
        "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
        "\n",
        "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
        "\n",
        "**User input or query** is typically a query directly input by the user of the system.\n",
        "\n",
        "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
        "\n",
        "Each of these components should usually be placed in the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2-VN6LTGBIl",
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:02:49.988305Z",
          "start_time": "2026-02-18T11:02:43.658966Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0299f92e-fbb0-4f50-bd63-748757993b4a"
      },
      "source": [
        "template = PromptTemplate.from_template(\"Write me an outline on {input_parameter}?\")\n",
        "user_input = input(\"Enter sport : \")\n",
        "prompt = template.format(input_parameter=user_input)\n",
        "print(\"Prompt :\",prompt)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter sport : basketball\n",
            "Prompt : Write me an outline on basketball?\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:05:23.649795Z",
          "start_time": "2026-02-18T11:05:21.023717Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "car25ltTM33D",
        "outputId": "2b536ff9-1f6b-4f07-e797-d033b62abd1e"
      },
      "source": [
        "prompt = PromptTemplate.from_template(\"Write me an outline on {input_parameter}?\")\n",
        "user_input = input(\"Enter sport : \")\n",
        "prompt.format(input_parameter=user_input)\n",
        "# Instantiation using initializer\n",
        "prompt = PromptTemplate(input_variables=[\"input_parameter\"], template=\"Write me an outline on {input_parameter}\")\n",
        "print(\"Prompt :\",prompt)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter sport : football\n",
            "Prompt : input_variables=['input_parameter'] input_types={} partial_variables={} template='Write me an outline on {input_parameter}'\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJmU_c8xM33D"
      },
      "source": [
        "We wouldn't typically know what the users prompt is beforehand, so we actually want to add this in. So rather than writing the prompt directly, we create a PromptTemplate with a single input variable query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT_DGdA9VGE8"
      },
      "source": [
        "## Chains\n",
        "\n",
        "Now we are going to use a Langchain concept Chains. Chains are responsible for the entire data flow inside Langchain. As we discussed above we are passing dynamic topic input variable to OpenAI. To accommodate this we will be using a chain called LLMChain. There are a bunch of chains supported in Langchain, we will talk about them later\n",
        "\n",
        "LLMChain takes the prompt from the prompt template we created above and fills it up with the dynamic input before passing to OpenAI LLM. Let's define LLMChain below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lszkNwMNVNiV",
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:05:26.917462Z",
          "start_time": "2026-02-18T11:05:26.914206Z"
        }
      },
      "source": [
        "# LangChain 1.0 LCEL syntax - use pipe operator instead of LLMChain\n",
        "chain = prompt | llm"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNRq_YnoQByB"
      },
      "source": [
        "Now that we have created a prompt template and a chain we can now input any topic we want. Instead of topic \"Tennis\" we can input \"Cricket\" or any other topic of your choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUh0JGOgPxyv",
        "outputId": "4aef7769-d5db-4fce-a146-e1049d69ec78",
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:05:40.631337Z",
          "start_time": "2026-02-18T11:05:28.309195Z"
        }
      },
      "source": [
        "result = chain.invoke({\"input_parameter\": \"Cricket\"})\n",
        "print(result)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Certainly! Here’s a detailed outline on cricket that covers various aspects of the game, including its history, rules, formats, and cultural impact.\\n\\n---\\n\\n### Outline on Cricket\\n\\n#### I. Introduction to Cricket\\n   A. Definition of Cricket\\n   B. Overview of its popularity and global reach\\n   C. Purpose of the outline\\n\\n#### II. History of Cricket\\n   A. Origins\\n      1. Early references in England (16th century)\\n      2. Evolution of the game\\n   B. Development of the rules\\n      1. Formation of the Marylebone Cricket Club (MCC)\\n      2. Key historical milestones (e.g., first Test match, first ODI)\\n   C. Global expansion\\n      1. Introduction to the British colonies\\n      2. Establishment of international competitions\\n\\n#### III. Basic Rules of Cricket\\n   A. Objective of the game\\n      1. Scoring runs\\n      2. Dismissing the opposing team\\n   B. The Field\\n      1. Description of the cricket field and pitch\\n      2. Key positions of players\\n   C. Innings\\n      1. Structure of innings\\n      2. Batting and bowling roles\\n   D. Scoring\\n      1. Runs, boundaries, and extras\\n      2. The concept of wickets and dismissals\\n   E. Umpiring and Decision Making\\n      1. Role of umpires\\n      2. Technology in decision making (DRS)\\n\\n#### IV. Formats of Cricket\\n   A. Test Cricket\\n      1. Explanation of the format\\n      2. Duration and structure\\n      3. Importance and prestige\\n   B. One Day Internationals (ODIs)\\n      1. Structure and rules\\n      2. Evolution and significance\\n   C. Twenty20 (T20) Cricket\\n      1. Fast-paced nature of the game\\n      2. Popularity and global leagues (e.g., IPL, BBL)\\n   D. Other Formats\\n      1. First-class cricket\\n      2. List A cricket\\n\\n#### V. Major International Competitions\\n   A. ICC Cricket World Cup\\n      1. History and significance\\n      2. Notable tournaments and winners\\n   B. ICC T20 World Cup\\n      1. Inception and development\\n      2. Key moments and teams\\n   C. Test Championships\\n      1. Structure and importance\\n      2. Notable achievements\\n   D. Regional tournaments and leagues\\n      1. Asia Cup, CPL, etc.\\n      2. Impact of domestic leagues\\n\\n#### VI. Key Players and Legends\\n   A. Historical Figures\\n      1. Sir Donald Bradman\\n      2. Sir Vivian Richards\\n   B. Modern Icons\\n      1. Sachin Tendulkar\\n      2. Brian Lara\\n      3. Virat Kohli\\n      4. Ben Stokes\\n   C. Women’s Cricket\\n      1. Pioneering players (e.g., Mithali Raj, Ellyse Perry)\\n      2. Growth and recognition\\n\\n#### VII. Cultural Impact of Cricket\\n   A. Cricket as a unifying force\\n      1. Role in society and communities\\n      2. National pride and identity\\n   B. Media and Broadcasting\\n      1. Evolution of cricket coverage\\n      2. Influence of digital media and social platforms\\n   C. Economic Impact\\n      1. Sponsorships and endorsements\\n      2. Tourism and infrastructure development\\n\\n#### VIII. Challenges and Future of Cricket\\n   A. Issues in governance and administration\\n   B. Balancing formats and player workload\\n   C. The role of technology and innovation\\n   D. Promoting inclusivity and diversity\\n\\n#### IX. Conclusion\\n   A. Recap of cricket’s significance\\n   B. Future prospects and potential developments\\n   C. Final thoughts on the enduring appeal of cricket\\n\\n---\\n\\nThis outline provides a comprehensive framework for discussing cricket, its history, rules, various formats, cultural significance, and future challenges. Each section can be expanded into more detailed content based on the specific focus or audience.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 879, 'prompt_tokens': 13, 'total_tokens': 892, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd4be55b21', 'id': 'chatcmpl-DDo1I2Fy7ZB8YcKFXMbfQaXb0HG3w', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c9e55-3ab1-7d40-a324-3e92b6d6e4e2-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 13, 'output_tokens': 879, 'total_tokens': 892, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVBNjz2OSzrs"
      },
      "source": [
        "Now let's extend it for a multi-input prompt. Let's generate an introductory paragraph to a blog post with variables title, audience and tone of voice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqVEYaPPGJuM",
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:05:48.082551Z",
          "start_time": "2026-02-18T11:05:48.077486Z"
        }
      },
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"title\", \"audience\", \"tone\"],\n",
        "    template=\"\"\"This program will generate an introductory paragraph to a blog post given a blog title, audience, and tone of voice\n",
        "\n",
        "    Blog Title: {title}\n",
        "    Audience: {audience}\n",
        "    Tone of Voice: {tone}\"\"\",\n",
        ")\n",
        "\n",
        "# LangChain 1.0 LCEL approach (recommended)\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Note: LLMChain is deprecated. The above LCEL syntax is the modern approach."
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJgvUP0pSVat",
        "outputId": "5ad1901f-80f8-48f4-9faa-b10c7fc4327c",
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:05:59.032977Z",
          "start_time": "2026-02-18T11:05:50.376838Z"
        }
      },
      "source": [
        "# LangChain 1.0: use invoke() instead of deprecated run()\n",
        "result = chain.invoke({\"title\": \"Best Activities in Toronto\", \"audience\": \"Millenials\", \"tone\": \"Lighthearted\"})\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey there, fellow adventurers! If you’re on the hunt for the ultimate guide to the coolest activities in Toronto, you’ve come to the right place. Whether you’re a foodie craving the latest brunch spot, an art lover looking to explore hidden galleries, or just someone who wants to soak up the vibrant city vibes, Toronto has got you covered. So grab your friends, put on your exploring shoes, and let’s dive into some of the best ways to experience this amazing city that’s bursting with energy and fun!\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP3dAgTgWq-r"
      },
      "source": [
        "## Combining Chains\n",
        "\n",
        "Often we would want to do multiple tasks using GPT. For example if we wish to generate an outline for a topic and use that outline to write a blog article we need to take the outline created from the first step and copy paste and paste as input to the second step\n",
        "\n",
        "Instead we can combine chains to achieve this in a single step. We will do this using a different type of chain called Sequential Chain. A sequential chain takes the output from one chain and passes on to the next. We will cover chains in more detail later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqHNoQ1OXLCC",
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:09.120471Z",
          "start_time": "2026-02-18T11:06:09.091988Z"
        }
      },
      "source": [
        "# LangChain 1.0 LCEL approach for chaining (recommended over SimpleSequentialChain)\n",
        "prompt_outline = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write me an outline on {topic}\",\n",
        ")\n",
        "\n",
        "llm = OpenAI(temperature=0.9, max_tokens=-1)\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"outline\"],\n",
        "    template=\"\"\"Write a blog article in the format of the given outline\n",
        "\n",
        "    Outline:\n",
        "    {outline}\"\"\",\n",
        ")\n",
        "\n",
        "# LCEL approach: chain prompts together using pipe operator\n",
        "# First chain generates outline, second uses it to write article\n",
        "def create_article_chain():\n",
        "    # Chain 1: Generate outline\n",
        "    outline_chain = prompt_outline | llm | StrOutputParser()\n",
        "\n",
        "    # Chain 2: Use outline to write article (need to wrap output in dict)\n",
        "    def format_outline(outline):\n",
        "        return {\"outline\": outline}\n",
        "\n",
        "    from langchain_core.runnables import RunnableLambda\n",
        "    article_chain = second_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Combined chain\n",
        "    return outline_chain | RunnableLambda(format_outline) | article_chain\n",
        "\n",
        "overall_chain = create_article_chain()"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIieZmmAXaCC",
        "outputId": "7bb84533-dcb2-4d1a-b564-591c86e3f0ed",
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:28.603820Z",
          "start_time": "2026-02-18T11:06:11.346811Z"
        }
      },
      "source": [
        "# LangChain 1.0: use invoke() with proper input format\n",
        "result = overall_chain.invoke({\"topic\": \"Tennis\"})\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Tennis: A Sport for All Ages\n",
            "\n",
            "Tennis is a game that has been played for centuries, with its origins traced back to 12th century France. Over the years, it has evolved into one of the most popular sports in the world, with millions of players and fans around the globe. Its popularity can be seen in the numerous professional tournaments and championships held annually, as well as the availability of facilities and clubs for amateur players. But what exactly is it about tennis that has captured the hearts of so many? In this article, we will dive into the basics of tennis and explore the physical and mental demands, as well as the rich history and famous rivalries that make this sport so beloved.\n",
            "\n",
            "Tennis Basics\n",
            "\n",
            "The game of tennis is played on a rectangular court with a net in the middle, using racquets and a ball. The court is divided into two halves by a net, and each player stands on one side. The objective of the game is to hit the ball over the net and into the opponent's side, within the boundaries of the court. Points are awarded when a player successfully hits a shot that is not returned by their opponent. A match is typically played in sets, with the first player to win two or three sets declared as the winner.\n",
            "\n",
            "Playing Techniques\n",
            "\n",
            "To be successful in tennis, a player needs to have a good grip on the racquet and be able to execute different types of shots. The five main shots in tennis are the serve, forehand, backhand, volley, and overhead. Each shot requires a specific technique and a combination of power and accuracy. Footwork and movement on the court are also crucial in order to reach and hit shots effectively.\n",
            "\n",
            "Singles and Doubles\n",
            "\n",
            "Tennis can be played in two different formats – singles and doubles. In singles, one player competes against another, while in doubles, two teams of two players each compete against each other. Doubles requires a different set of strategies and communication between partners, while singles is more focused on individual tactics and strengths.\n",
            "\n",
            "Physical and Mental Demands\n",
            "\n",
            "Tennis is a physically demanding sport that requires players to have a good level of fitness and stamina. The constant movement and quick changes in direction on the court, along with the strength and precision needed for shots, make it a great form of exercise. In addition to physical strength, tennis also requires mental strength. Being able to stay focused, maintain composure under pressure, and make quick decisions are essential for success in the game.\n",
            "\n",
            "Professional Tennis\n",
            "\n",
            "The professional tennis circuit is made up of various tournaments and championships held around the world, including the four Grand Slams – the Australian Open, French Open, Wimbledon, and US Open. These events attract the world's top players, who showcase their skills and battle it out for the coveted titles. Some notable players include Roger Federer, Rafael Nadal, and Serena Williams, who have all achieved remarkable records and have unique playing styles.\n",
            "\n",
            "Tennis as a Lifetime Sport\n",
            "\n",
            "One of the great things about tennis is that it can be played by people of all ages. Whether you're a young child just starting out, or a senior looking for a low-impact form of exercise, tennis has something to offer. It is also a sport that can be played throughout one's life, with many facilities and clubs offering lessons and programs for all age groups.\n",
            "\n",
            "Etiquette and Sportsmanship\n",
            "\n",
            "Tennis is not just about the physical and technical aspects of the game, but also about having good sportsmanship and following proper etiquette. Players are expected to show respect for their opponents and officials, and to exhibit good behavior on the court. This includes acknowledging a good shot by your opponent and following the rules and code of conduct.\n",
            "\n",
            "Technology and Evolution of Tennis\n",
            "\n",
            "As with most sports, technology has had a significant impact on tennis. From the development of more advanced racquets and strings to the introduction of electronic line calling systems, technology has helped improve the game in many ways. However, there have also been debates surrounding its influence, such as the controversy over the use of performance-enhancing equipment.\n",
            "\n",
            "Famous Tennis Rivalries\n",
            "\n",
            "Throughout the years, tennis has seen many memorable rivalries that have captivated fans and brought attention to the sport. From the historical battles between John McEnroe and Bjorn Borg, to the modern-day rivalry of Roger Federer and Rafael Nadal, these matchups have pushed players to their limits and showcased the true essence of competition.\n",
            "\n",
            "Conclusion\n",
            "\n",
            "In conclusion, tennis is a sport that offers something for everyone – whether as a player or a spectator. Its rich history, physical and mental demands, and famous rivalries make it an exciting and captivating sport. So why not pick up a racquet and give it a try? Who knows, you may just fall in love with the game that has been enjoyed by people of all ages for centuries.\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq_g2hOHM33F"
      },
      "source": [
        "## Revisit the Prompt in Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FGv98DAM33F"
      },
      "source": [
        "The prompt template classes in Langchain are built to make constructing prompts with dynamic inputs easier. Of these classes, the simplest is the PromptTemplate. We’ll test this by adding a single dynamic input to our previous prompt, the user query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:38.484629Z",
          "start_time": "2026-02-18T11:06:38.479879Z"
        },
        "id": "dT5Hcv2wM33F"
      },
      "source": [
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-_aPx9PM33F"
      },
      "source": [
        "With this, we can use the **format** method on our **prompt_template** to see the effect of passing a query to the template."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:39.781285Z",
          "start_time": "2026-02-18T11:06:39.778394Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBuKJFbAM33F",
        "outputId": "19eef584-231c-471a-9f8e-aabe2d3d4d55"
      },
      "source": [
        "print(\n",
        "    prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\"\n",
        "    )\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
            "Their superior performance over smaller models has made them incredibly\n",
            "useful for developers building NLP enabled applications. These models\n",
            "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
            "using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: Which libraries and model providers offer LLMs?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:43.276260Z",
          "start_time": "2026-02-18T11:06:42.914811Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6W5ih78eM33F",
        "outputId": "6cb9ffdf-a2c7-4d6e-9687-fa98dfdd6aaf"
      },
      "source": [
        "prompt = prompt_template.format(query=\"Which libraries and model providers offer LLMs?\")\n",
        "chain = prompt_template | llm\n",
        "chain.invoke(prompt)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hugging Face's `transformers` library, `openai` library, and `cohere` library offer LLMs.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeQ_1WCcM33F"
      },
      "source": [
        "### Few Shot Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk_3A0gsM33F"
      },
      "source": [
        "The success of LLMs comes from their large size and ability to store “knowledge” within the model parameter, which is learned during model training. However, there are more ways to pass knowledge to an LLM. The two primary methods are:\n",
        "\n",
        "- Parametric knowledge — the knowledge mentioned above is anything that has been learned by the model during training time and is stored within the model weights (or parameters).\n",
        "- Source knowledge — any knowledge provided to the model at inference time via the input prompt.\n",
        "Langchain’s FewShotPromptTemplate caters to source knowledge input. The idea is to “train” the model on a few examples — we call this few-shot learning — and these examples are given to the model within the prompt.\n",
        "\n",
        "Few-shot learning is perfect when our model needs help understanding what we’re asking it to do. We can see this in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:48.159899Z",
          "start_time": "2026-02-18T11:06:48.154634Z"
        },
        "id": "e3ZqRtAqM33G"
      },
      "source": [
        "# create our examples\n",
        "examples = [ #you can have n such example of query-answer pairs\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create a example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ8q5EKTM33G"
      },
      "source": [
        "If we then pass in the examples and user query, we will get this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:50.333170Z",
          "start_time": "2026-02-18T11:06:50.329648Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIRzlW6TM33G",
        "outputId": "87962fcc-b648-4061-b8ac-6dbafd75c35c"
      },
      "source": [
        "query = \"What is the meaning of life?\"\n",
        "\n",
        "print(few_shot_prompt_template.format(query=query))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: \n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeAhtN5bM33G"
      },
      "source": [
        "Considering this, we need to balance the number of examples included and our prompt size. Our hard limit is the maximum context size, but we must also consider the cost of processing more tokens through the LLM. Fewer tokens mean a cheaper service and faster completions from the LLM.\n",
        "\n",
        "The `FewShotPromptTemplate` allows us to vary the number of examples included based on these variables. First, we create a more extensive list of examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:52.942589Z",
          "start_time": "2026-02-18T11:06:52.937702Z"
        },
        "id": "kZtzW-KeM33G"
      },
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkc0ZGYUM33G"
      },
      "source": [
        "After this, rather than passing the examples directly, we actually use a `LengthBasedExampleSelector` like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:54.845689Z",
          "start_time": "2026-02-18T11:06:54.841968Z"
        },
        "id": "dKVSy5LCM33G"
      },
      "source": [
        "# LangChain 1.0: import from langchain_core\n",
        "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=50  # this sets the max length that examples should be\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cboGyEuyM33G"
      },
      "source": [
        "We then pass our `example_selector` to the `FewShotPromptTemplate` to create a new — and dynamic — prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:56.405298Z",
          "start_time": "2026-02-18T11:06:56.401166Z"
        },
        "id": "oydsOrqXM33G"
      },
      "source": [
        "# now create the few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # use example_selector instead of examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyce8iKpM33H"
      },
      "source": [
        "Now if we pass a shorter or longer query, we should see that the number of included examples will vary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:06:58.251289Z",
          "start_time": "2026-02-18T11:06:58.247494Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSJyskfHM33H",
        "outputId": "70d2bb27-f5de-4810-e379-1288f71a18b1"
      },
      "source": [
        "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: How do birds fly?\n",
            "AI: \n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q96QuhpbM33H"
      },
      "source": [
        "Passing a longer question will result in fewer examples being included:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:07:00.521250Z",
          "start_time": "2026-02-18T11:07:00.519422Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qtk1GGDM33H",
        "outputId": "a81c1a91-9f72-4957-fa4c-c34b7cec502a"
      },
      "source": [
        "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
        "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
        "what is the best way to do that?\"\"\"\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: If I am in America, and I want to call someone in another country, I'm\n",
            "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
            "what is the best way to do that?\n",
            "AI: \n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOVJ8FSfM33I"
      },
      "source": [
        "With this, we’re returning fewer examples within the prompt variable. Allowing us to limit excessive token usage and avoid errors from surpassing the maximum context window of the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8DVwQZGM33I"
      },
      "source": [
        "An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3yRSledM33I"
      },
      "source": [
        "## Let's revisit chains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:07:03.094421Z",
          "start_time": "2026-02-18T11:07:03.089898Z"
        },
        "id": "50qYDZl6M33I"
      },
      "source": [
        "# LangChain 1.0: Updated token counting function\n",
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        # Use invoke() instead of deprecated run()\n",
        "        if isinstance(query, dict):\n",
        "            result = chain.invoke(query)\n",
        "        else:\n",
        "            result = chain.invoke({\"question\": query} if hasattr(chain, 'input_keys') else query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnMa2tXSM33I"
      },
      "source": [
        "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
        "\n",
        "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
        "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wbE5USPM33I"
      },
      "source": [
        "### Utility Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEpfbUUlM33I"
      },
      "source": [
        "Let's start with a simple utility chain. The `LLMMathChain` gives llms the ability to do math. Let's see how it works!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numexpr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVsk4Iy3V8lC",
        "outputId": "3c600b60-428c-4a7c-fb51-17383364181c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.12/dist-packages (2.14.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from numexpr) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:07:05.780010Z",
          "start_time": "2026-02-18T11:07:05.343074Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS1vO8UXM33I",
        "outputId": "d4771ad6-45ab-4992-f0cf-80f482a99e9d"
      },
      "source": [
        "llm_math = LLMMathChain.from_llm(llm=llm)\n",
        "\n",
        "\n",
        "count_tokens(llm_math, \"What is 13 raised to the .3432 power?\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 226 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is 13 raised to the .3432 power?',\n",
              " 'answer': 'Answer: 2.4116004626599237'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPK19M1DM33I"
      },
      "source": [
        "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcmM4gI_M33I"
      },
      "source": [
        "**Enter prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcwRk7q5M33I"
      },
      "source": [
        "The question we send as input to the chain is not the only input that the llm recieves 😉. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:07:08.781255Z",
          "start_time": "2026-02-18T11:07:08.777582Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ClT4n0HM33I",
        "outputId": "e88e4327-6209-4d46-8881-12ee1bffc050"
      },
      "source": [
        "print(llm_math.prompt.template)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
            "\n",
            "Question: ${{Question with math problem.}}\n",
            "```text\n",
            "${{single line mathematical expression that solves the problem}}\n",
            "```\n",
            "...numexpr.evaluate(text)...\n",
            "```output\n",
            "${{Output of running the code}}\n",
            "```\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Begin.\n",
            "\n",
            "Question: What is 37593 * 67?\n",
            "```text\n",
            "37593 * 67\n",
            "```\n",
            "...numexpr.evaluate(\"37593 * 67\")...\n",
            "```output\n",
            "2518731\n",
            "```\n",
            "Answer: 2518731\n",
            "\n",
            "Question: 37593^(1/5)\n",
            "```text\n",
            "37593**(1/5)\n",
            "```\n",
            "...numexpr.evaluate(\"37593**(1/5)\")...\n",
            "```output\n",
            "8.222831614237718\n",
            "```\n",
            "Answer: 8.222831614237718\n",
            "\n",
            "Question: {question}\n",
            "\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE8ILv6lM33I"
      },
      "source": [
        "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! 🧐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:07:11.326815Z",
          "start_time": "2026-02-18T11:07:10.959536Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWcYNw7nM33I",
        "outputId": "f6d65d71-edb0-4135-c5cb-19c7532d7598"
      },
      "source": [
        "# LangChain 1.0 LCEL approach (no LLMChain needed)\n",
        "prompt = PromptTemplate(input_variables=['question'], template='{question}')\n",
        "llm_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# we ask the llm for the answer with no context\n",
        "result = count_tokens(llm_chain, {\"question\": \"What is 13 raised to the .3432 power?\"})\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 29 tokens\n",
            "\n",
            "\n",
            "13 raised to the .3432 power is approximately 2.7697.\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PorwTHWUM33I"
      },
      "source": [
        "Wrong answer! Herein lies the power of prompting and one of our most important insights so far:\n",
        "\n",
        "**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWKVU1OQM33I"
      },
      "source": [
        "Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:07:49.403974Z",
          "start_time": "2026-02-18T11:07:49.391944Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mFVjUDdM33I",
        "outputId": "3878c154-a938-4b7c-b84d-2436637ecbc0"
      },
      "source": [
        "print(inspect.getsource(llm_math._call))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    def _call(\n",
            "        self,\n",
            "        inputs: dict[str, str],\n",
            "        run_manager: CallbackManagerForChainRun | None = None,\n",
            "    ) -> dict[str, str]:\n",
            "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
            "        _run_manager.on_text(inputs[self.input_key])\n",
            "        llm_output = self.llm_chain.predict(\n",
            "            question=inputs[self.input_key],\n",
            "            stop=[\"```output\"],\n",
            "            callbacks=_run_manager.get_child(),\n",
            "        )\n",
            "        return self._process_llm_result(llm_output, _run_manager)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPzdsXiLM33J"
      },
      "source": [
        "So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator. We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhs_GW15M33J"
      },
      "source": [
        "Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXKEnOxuM33J"
      },
      "source": [
        "Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly.\n",
        "\n",
        "The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check out the LangChain documentation](https://python.langchain.com/docs/).\n",
        "\n",
        "**Note on Agents (LangChain 1.0):** For complex agent workflows that require:\n",
        "- Multi-step reasoning\n",
        "- Tool selection and execution\n",
        "- State management across interactions\n",
        "\n",
        "Consider using **[LangGraph](https://langchain-ai.github.io/langgraph/)** which provides:\n",
        "- Graph-based orchestration for agent workflows\n",
        "- Built-in state management\n",
        "- Better control over agent execution flow\n",
        "\n",
        "*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by2DgcIBM33J"
      },
      "source": [
        "### Generic chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAkeXxnCM33J"
      },
      "source": [
        "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GF_Rc2bM33J"
      },
      "source": [
        "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat 😉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv4rIh9TM33J"
      },
      "source": [
        "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:09.461709Z",
          "start_time": "2026-02-18T11:08:09.456498Z"
        },
        "id": "XcRn9RiuM33J"
      },
      "source": [
        "def transform_func(inputs: dict) -> dict:\n",
        "    text = inputs[\"text\"]\n",
        "\n",
        "    # replace multiple new lines and multiple spaces with a single one\n",
        "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
        "    text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "    return {\"output_text\": text}"
      ],
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HYcGW8BM33J"
      },
      "source": [
        "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:11.076939Z",
          "start_time": "2026-02-18T11:08:11.073486Z"
        },
        "id": "XnDEkxZlM33J"
      },
      "source": [
        "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"
      ],
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:12.642822Z",
          "start_time": "2026-02-18T11:08:12.633404Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auqz_Lk3M33J",
        "outputId": "4cd42350-50cc-420e-a898-ffaf93695b3d"
      },
      "source": [
        "# Use invoke() instead of deprecated run()\n",
        "clean_extra_spaces_chain.invoke({'text': 'A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.'})"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.',\n",
              " 'output_text': 'A random text with some irregular spacing.\\n Another one here as well.'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9wIE1zHM33J"
      },
      "source": [
        "Great! Now things will get interesting.\n",
        "\n",
        "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MEW7nyFM33J"
      },
      "source": [
        "First we will build the prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:29.042442Z",
          "start_time": "2026-02-18T11:08:29.035868Z"
        },
        "id": "TnqUaSGGM33J"
      },
      "source": [
        "template = \"\"\"Paraphrase this text:\n",
        "\n",
        "{output_text}\n",
        "\n",
        "In the style of a {style}.\n",
        "\n",
        "Paraphrase: \"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
      ],
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FL7w692M33J"
      },
      "source": [
        "And next, initialize our chain:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:33.413043Z",
          "start_time": "2026-02-18T11:08:33.409240Z"
        },
        "id": "9o0p3abLM33J"
      },
      "source": [
        "# LangChain 1.0 LCEL approach\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "style_paraphrase_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Note: With LCEL, we handle variable naming differently using RunnableLambda"
      ],
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYzaFmhFM33J"
      },
      "source": [
        "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
        "\n",
        "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_NQFSgKM33J"
      },
      "source": [
        "Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:36.155117Z",
          "start_time": "2026-02-18T11:08:36.149160Z"
        },
        "id": "MkKCjsHxM33J"
      },
      "source": [
        "# LangChain 1.0 LCEL approach for sequential chains\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "# Create the sequential chain using LCEL\n",
        "def create_styled_paraphrase_chain(style: str):\n",
        "    # Step 1: Clean the text\n",
        "    clean_step = RunnableLambda(transform_func)\n",
        "\n",
        "    # Step 2: Apply style paraphrasing\n",
        "    style_prompt = PromptTemplate(\n",
        "        input_variables=[\"output_text\", \"style\"],\n",
        "        template=\"\"\"Paraphrase this text:\n",
        "\n",
        "{output_text}\n",
        "\n",
        "In the style of a {style}.\n",
        "\n",
        "Paraphrase: \"\"\"\n",
        "    )\n",
        "\n",
        "    # Combine steps\n",
        "    def add_style(cleaned):\n",
        "        return {\"output_text\": cleaned[\"output_text\"], \"style\": style}\n",
        "\n",
        "    return (\n",
        "        RunnableLambda(lambda x: {\"text\": x[\"text\"]})\n",
        "        | clean_step\n",
        "        | RunnableLambda(add_style)\n",
        "        | style_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "# Create chain for 90s rapper style\n",
        "sequential_chain = create_styled_paraphrase_chain(\"a 90s rapper\")"
      ],
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8pGGAGlM33K"
      },
      "source": [
        "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:42.586605Z",
          "start_time": "2026-02-18T11:08:42.582767Z"
        },
        "id": "db-H_2DaM33K"
      },
      "source": [
        "input_text = \"\"\"\n",
        "Chains allow us to combine multiple\n",
        "\n",
        "\n",
        "components together to create a single, coherent application.\n",
        "\n",
        "For example, we can create a chain that takes user input,       format it with a PromptTemplate,\n",
        "\n",
        "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by\n",
        "\n",
        "\n",
        "combining chains with other components.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MFVg17-M33K"
      },
      "source": [
        "We are all set. Time to get creative!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:45.286278Z",
          "start_time": "2026-02-18T11:08:44.073390Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbSIxjcnM33K",
        "outputId": "ecc3e9a2-970f-46d3-852d-0dfd39f87185"
      },
      "source": [
        "# LangChain 1.0: use invoke()\n",
        "result = sequential_chain.invoke({'text': input_text})\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yo, check it - chains be the key to makin' one dope app! Like, you can link user input with a fly PromptTemplate, then pass it to an LLM. And yo, if you wanna get even crazier, just combine a buncha chains or mix 'em with other components. That's how we roll. \n"
          ]
        }
      ],
      "execution_count": 35
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R50_IX6SM33L"
      },
      "source": [
        "# LangChain's Chains Operations\n",
        "\n",
        "In LangChain, chains are a core concept used to combine different operations in a sequential or conditional manner. These chains allow you to build complex workflows by linking various components together. Here are some of the main types of chain operations and how they can be used:\n",
        "\n",
        "## 1. SequentialChain\n",
        "\n",
        "`SequentialChain` is used to link multiple components together so that the output of one component becomes the input for the next. This is useful for creating multi-step processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4YGH8V5M33L",
        "outputId": "5c1a4843-2a4f-4477-f25f-9d04b4baf322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.10)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.13 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.2.16)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.23.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.7.6)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (26.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (9.1.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.14.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.67.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=2.20.0->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=2.20.0->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        " !pip install -U langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:56.991256Z",
          "start_time": "2026-02-18T11:08:52.652256Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d9SzoECM33L",
        "outputId": "589ba0b0-c647-4b73-acdc-c03835bf5f3c"
      },
      "source": [
        "# LangChain 1.0 LCEL approach for SequentialChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define prompts\n",
        "prompt1 = PromptTemplate(template=\"Translate English to French: {text}\", input_variables=[\"text\"])\n",
        "prompt2 = PromptTemplate(template=\"Translate French to Spanish: {french_text}\", input_variables=[\"french_text\"])\n",
        "\n",
        "# Use ChatOpenAI\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# LangChain 1.0 LCEL: Chain prompts together\n",
        "chain = (\n",
        "    prompt1\n",
        "    | llm\n",
        "    | output_parser\n",
        "    | RunnableLambda(lambda x: {\"french_text\": x})\n",
        "    | prompt2\n",
        "    | llm\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "# Run the chain\n",
        "result = chain.invoke({\"text\": \"The house is wonderful.\"})\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La casa es maravillosa.\n"
          ]
        }
      ],
      "execution_count": 37
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-5vAjUsM33L"
      },
      "source": [
        "## 2. LLMChain\n",
        "This is one of the most commonly used chains. It's a chain that combines a language model (LLM) with a prompt template.  It is used when you want to execute a single step, such as summarization, translation, or Q&A.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:08:59.715004Z",
          "start_time": "2026-02-18T11:08:56.999208Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjgQ9CfDM33L",
        "outputId": "2670f683-9f78-4f9c-9b80-0b1fc1a8d221"
      },
      "source": [
        "# LangChain 1.0 LCEL approach (instead of LLMChain)\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = PromptTemplate(template=\"Summarize the following text in 10 words or fewer: {text}\", input_variables=[\"text\"])\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\")\n",
        "\n",
        "# LangChain 1.0 LCEL chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "text = \"\"\"\n",
        "LangChain is an open-source framework designed to facilitate the development of applications using large language models (LLMs).\n",
        "It provides tools to integrate various components such as prompt templates, memory, chains, and agents.\n",
        "LangChain enables developers to build chatbots, question-answering systems, and other AI-powered applications efficiently.\n",
        "\"\"\"\n",
        "\n",
        "result = chain.invoke({\"text\": text})\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain: Open-source framework for developing LLM-based applications.\n"
          ]
        }
      ],
      "execution_count": 38
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfoBLGMUM33L"
      },
      "source": [
        "## 3. LLMRouterChain\n",
        "\n",
        "`LLMRouterChain` helps route queries to the most appropriate large language model (LLM) or tool based on the input. It is particularly useful when working with multiple models or APIs with different capabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:09:06.458421Z",
          "start_time": "2026-02-18T11:09:05.264827Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LvIv34KM33L",
        "outputId": "6007d210-c829-410d-ec46-3fe4f490373a"
      },
      "source": [
        "# LangChain 1.0: Router chains\n",
        "# Note: For complex routing logic, consider using LangGraph for better control\n",
        "\n",
        "from langchain_classic.chains.router.multi_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "\n",
        "destinations = \"\"\"\n",
        "animals: prompt for animal expert\n",
        "vegetables: prompt for a vegetable expert\n",
        "\"\"\"\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)\n",
        "\n",
        "print(router_template.replace(\"`\", \"'\"))  # for rendering purposes\n",
        "\n",
        "\n",
        "from langchain_classic.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "# Note: For production use cases with complex routing,\n",
        "# consider LangGraph which provides more control over routing logic"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
            "\n",
            "<< FORMATTING >>\n",
            "Return a markdown code snippet with a JSON object formatted to look like:\n",
            "'''json\n",
            "{{\n",
            "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
            "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
            "}}\n",
            "'''\n",
            "\n",
            "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
            "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
            "\n",
            "<< CANDIDATE PROMPTS >>\n",
            "\n",
            "animals: prompt for animal expert\n",
            "vegetables: prompt for a vegetable expert\n",
            "\n",
            "\n",
            "<< INPUT >>\n",
            "{input}\n",
            "\n",
            "<< OUTPUT (must include '''json at the start of the response) >>\n",
            "<< OUTPUT (must end with ''') >>\n",
            "\n"
          ]
        }
      ],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:09:17.391061Z",
          "start_time": "2026-02-18T11:09:15.346416Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Noinf-_6M33L",
        "outputId": "f56209bf-55aa-4d3e-8cb6-87637e903e04"
      },
      "source": [
        "result = chain.invoke({\"input\": \"What color are carrots?\"})\n",
        "\n",
        "print(result[\"destination\"])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vegetables\n"
          ]
        }
      ],
      "execution_count": 40
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQTS82_DM33M"
      },
      "source": [
        "## 4. RunnableParallel\n",
        "`RunnableParallel` RunnableParallel allows running multiple chains, functions, or callables in parallel and returns a dictionary of results. It is part of the Runnable framework, which provides more flexibility and better performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:09:21.315481Z",
          "start_time": "2026-02-18T11:09:19.624060Z"
        },
        "id": "QNkOp3GXM33M"
      },
      "source": [
        "# LangChain 1.0 LCEL: RunnableParallel for parallel execution\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Create prompts\n",
        "prompt_1 = PromptTemplate(input_variables=[\"question\"], template=\"Translate this to French: {question}\")\n",
        "prompt_2 = PromptTemplate(input_variables=[\"question\"], template=\"Translate this to Spanish: {question}\")\n",
        "\n",
        "# LangChain 1.0 LCEL chains\n",
        "chain_1 = prompt_1 | llm | output_parser\n",
        "chain_2 = prompt_2 | llm | output_parser\n",
        "\n",
        "# Run both chains in parallel using RunnableParallel\n",
        "parallel_chain = RunnableParallel(french=chain_1, spanish=chain_2)\n",
        "\n",
        "# Example input\n",
        "result = parallel_chain.invoke({\"question\": \"Hello, how are you?\"})\n",
        "\n",
        "print(result)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfhB92aPM33M"
      },
      "source": [
        "## 5. MapReduceDocumentsChain\n",
        "`MapReduceChain` is a chain in LangChain used for processing large numbers of documents efficiently. It applies a map-reduce approach, where:\n",
        "\n",
        "1- Map Step: Each document is processed independently, generating intermediate responses.\n",
        "\n",
        "2- Reduce Step: These intermediate responses are aggregated to produce the final output.\n",
        "\n",
        "This chain is useful for summarization, QA over large documents, and other NLP tasks requiring multi-document processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:09:29.608758Z",
          "start_time": "2026-02-18T11:09:29.599762Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxv6Rx4jM33M",
        "outputId": "c636a55c-bffc-44a8-f0f7-01814a2550d6"
      },
      "source": [
        "# LangChain 1.0: MapReduceDocumentsChain\n",
        "# Note: For complex document processing workflows, consider using LangGraph\n",
        "\n",
        "from langchain_classic.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain_classic.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=\"Apples are red\", metadata={\"title\": \"apple_book\"}),\n",
        "    Document(page_content=\"Blueberries are blue\", metadata={\"title\": \"blueberry_book\"}),\n",
        "    Document(page_content=\"Bananas are yelow\", metadata={\"title\": \"banana_book\"}),\n",
        "]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Map\n",
        "map_template = \"Write a concise summary of the following: {docs}.\"\n",
        "map_prompt = ChatPromptTemplate([(\"human\", map_template)])\n",
        "\n",
        "# For LCEL, we can create a simple map chain\n",
        "map_chain = map_prompt | llm\n",
        "\n",
        "# Reduce\n",
        "reduce_template = \"\"\"\n",
        "The following is a set of summaries:\n",
        "{docs}\n",
        "Take these and distill it into a final, consolidated summary\n",
        "of the main themes.\n",
        "\"\"\"\n",
        "reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
        "reduce_chain = reduce_prompt | llm\n",
        "\n",
        "# Legacy approach using MapReduceDocumentsChain (still works but LCEL preferred)\n",
        "from langchain_classic.chains.llm import LLMChain\n",
        "\n",
        "map_llm_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "reduce_llm_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_llm_chain, document_variable_name=\"docs\"\n",
        ")\n",
        "\n",
        "# Combines and iteratively reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    token_max=1000,\n",
        ")\n",
        "\n",
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    llm_chain=map_llm_chain,\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    document_variable_name=\"docs\",\n",
        "    return_intermediate_steps=False,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8891/935411537.py:39: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  map_llm_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
            "/tmp/ipython-input-8891/935411537.py:43: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
            "  combine_documents_chain = StuffDocumentsChain(\n",
            "/tmp/ipython-input-8891/935411537.py:48: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n",
            "  reduce_documents_chain = ReduceDocumentsChain(\n",
            "/tmp/ipython-input-8891/935411537.py:55: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n",
            "  map_reduce_chain = MapReduceDocumentsChain(\n"
          ]
        }
      ],
      "execution_count": 41
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-18T11:09:38.799900Z",
          "start_time": "2026-02-18T11:09:32.344044Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hm6vG9VZM33M",
        "outputId": "b19a1b57-a451-48de-a26d-5e48cec4bdeb"
      },
      "source": [
        "result = map_reduce_chain.invoke(documents)\n",
        "\n",
        "print(result[\"output_text\"])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fruits come in various colors, with apples being red, blueberries blue, and bananas yellow.\n"
          ]
        }
      ],
      "execution_count": 42
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6jfwr8LM33M"
      },
      "source": [
        "These are some of the primary chain operations available in LangChain. They can be combined and customized to create complex workflows tailored to specific language processing tasks.\n",
        "\n",
        "## LangChain 1.0 Recommendations\n",
        "\n",
        "| Use Case | Recommended Approach |\n",
        "|----------|---------------------|\n",
        "| Simple chains | **LCEL** with `\\|` pipe operator |\n",
        "| Parallel execution | **RunnableParallel** |\n",
        "| Complex agent workflows | **[LangGraph](https://langchain-ai.github.io/langgraph/)** |\n",
        "| Multi-step reasoning | **LangGraph** with state management |\n",
        "| Document processing | **LCEL** or legacy chains |\n",
        "\n",
        "**Key Changes in LangChain 1.0:**\n",
        "- `LLMChain` is deprecated - use LCEL instead\n",
        "- Use `invoke()` instead of `run()`\n",
        "- Imports organized into `langchain_openai`, `langchain_community`, `langchain_core`\n",
        "- For agents, transition to **LangGraph** for better orchestration"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "83Xf6VuJcII8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_classic.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Tell me a short fact about {topic}\"\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(llm_chain.invoke({\"topic\": \"Tennis\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTxitY8IY97J",
        "outputId": "2aa23669-1b03-41e6-8329-414fec292ab1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'topic': 'Tennis', 'text': 'Tennis originated in France in the late 12th century, where players used their hands to hit a ball back and forth, a game known as \"jeu de paume.\" The use of rackets evolved later in the 16th century.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import SimpleSequentialChain\n",
        "prompt1 = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Give me a short title about {topic}\"\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    input_variables=[\"title\"],\n",
        "    template=\"Write a short paragraph about {title}\"\n",
        ")\n",
        "\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
        "\n",
        "simple_chain = SimpleSequentialChain(chains=[chain1, chain2])\n",
        "\n",
        "print(simple_chain.run(\"Tennis\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mx-N6hCZCzv",
        "outputId": "7862ac26-9b1b-4840-d8c1-32a3b05a6b41"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Game, Set, Match: The Art of Tennis\" explores the intricate interplay between athleticism, strategy, and elegance that defines the sport of tennis. It delves into the technical precision required for each stroke, the mental fortitude necessary to outmaneuver opponents, and the artistry involved in shaping a player's unique style. From the graceful footwork of a baseline rally to the explosive power of a serve, the book captures the essence of tennis as not just a game but a refined expression of human potential. Through vivid storytelling and insightful analysis, it celebrates the athletes who transform the court into a stage where every match is a masterpiece waiting to unfold.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import SequentialChain\n",
        "prompt1 = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Give me a short title about {topic}\"\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    input_variables=[\"title\"],\n",
        "    template=\"Write a short paragraph about {title}\"\n",
        ")\n",
        "\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"title\")\n",
        "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"paragraph\")\n",
        "\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[chain1, chain2],\n",
        "    input_variables=[\"topic\"],\n",
        "    output_variables=[\"title\", \"paragraph\"]\n",
        ")\n",
        "\n",
        "print(sequential_chain({\"topic\": \"Tennis\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qU6KpPGZFF3",
        "outputId": "3a4fb54c-234c-4060-d839-ab03cadbca64"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'topic': 'Tennis', 'title': '\"Rally of Champions: The Art of Tennis\"', 'paragraph': '\"Rally of Champions: The Art of Tennis\" is a captivating exploration of the sport\\'s rich history and the artistry behind its greatest moments. This event celebrates the skill, precision, and athleticism that define tennis, showcasing legendary matches and iconic players who have transformed the game into a breathtaking spectacle. Through a blend of expert analysis, personal anecdotes, and visual storytelling, the rally honors the strategic nuances and emotional depth that make tennis a beloved sport worldwide. Whether you\\'re a lifelong fan or a newcomer to the game, \"Rally of Champions\" invites you to appreciate the intricate ballet of movement, strategy, and raw talent that elevates tennis to an art form.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJPURpSDdR0i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "consulting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}